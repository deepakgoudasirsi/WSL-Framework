\documentclass{ieeeaccess}
\usepackage{cite}
\renewcommand\citedash{-}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{array}
\usepackage{booktabs}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\history{Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.}
\doi{10.1109/ACCESS.2025.3401234}

\title{Multilingual Multimodal Meme Classification: Detecting Hate and Extending to Zero-Shot Classification}
\author{\uppercase{Pragathi BC}\authorrefmark{1} \uppercase{Neha N}\authorrefmark{2} \uppercase{Akanksha Pai}\authorrefmark{3} \uppercase{Amit Chaulwar}\authorrefmark{4} \uppercase{Shanta Rangaswamy}\authorrefmark{5} \uppercase{Jayanthi P N}\authorrefmark{6}}
\address[1]{Computer Science and Engineering, RV College of Engineering®, Bengaluru, India}
\address[2]{Computer Science and Engineering, RV College of Engineering®, Bengaluru, India}
\address[3]{Computer Science and Engineering, RV College of Engineering®, Bengaluru, India}
\address[4]{Samsung India Ltd, Bengaluru, India}
\address[5]{Computer Science and Engineering, RV College of Engineering®, Bengaluru, India}
\address[6]{Computer Science and Engineering, RV College of Engineering®, Bengaluru, India}
\tfootnote{This work was supported in part by the RV College of Engineering®, Bengaluru, India and Samsung India Ltd under the collaborative research program.}

\markboth
{Pragathi \headeretal: Multilingual Multimodal Meme Classification}
{Pragathi \headeretal: Multilingual Multimodal Meme Classification}

\corresp{Corresponding author: Shanta Rangaswamy (e-mail: shantharangaswamy@rvce.edu.in).}

\begin{abstract}
The ubiquity of social media has revolutionized communication, but it has also facilitated the spread of offensive content in the form of hate memes. The detection of such harmful content in low-resource languages like Tamil, Bengali and Hindi presents unique challenges, considering cultural nuances, linguistic intricacies and lack of large annotated data. This work investigates hate meme identification using memes dataset for Indian native languages, annotated with hate and no-hate labels. To tackle the complexities of cross-lingual and multimodal analysis, we generate captions describing the memes and harness the LASER (Language-Agnostic Sentence Representations) model to derive language-agnostic embeddings for the captions and text in the memes. These embeddings are concatenated and used for the training of classifier models, such as SVM, Random Forest, or neural networks, to effectively detect hate memes. This work endeavors to foster safer online spaces for low-resource language speakers (e.g. Tamil, Hindi, Bengali) by identifying and countering offensive content within those languages. Therefore, we extend our investigation to zero-shot classification, so that the proposed methodology can also be used for hate meme detection in low-resource languages without data.
\end{abstract}

\begin{keywords}
Multimodal, Cross-Lingual, LASER, Zero Shot Classification, Hate Detection, Meme Classification, Low-Resource Languages, Machine Learning
\end{keywords}

\titlepgskip=-15pt

\maketitle

\section{Introduction}
\label{sec:introduction}

The rapid growth of social media and meme culture has transformed the way information is shared and communicated, facilitating the spread of ideas and humor across diverse communities. However, this phenomenon has also led to the widespread dissemination of offensive and harmful content in the form of hate memes. Hate memes combine visual imagery and textual elements to convey discriminatory or offensive messages, often in a seemingly humorous or satirical manner. Detecting and mitigating such content is crucial to fostering a safer and more inclusive online space for all users, especially in Indian native languages, where hate memes may take on specific linguistic subtleties and cultural contexts.

Existing hate speech detection techniques often rely on monolingual unimodal models, which may not effectively handle the complexity of detecting offensive memes in low-resource languages like Indian native languages. The challenges lie in the multimodal nature of memes, where the interplay between visual and textual elements can significantly affect the overall meaning and impact of the content. Furthermore, cross-lingual understanding is essential in a multilingual country like India, where memes can be shared across diverse linguistic communities.

To address these challenges, we propose a novel approach that leverages the LASER (Language-Agnostic Sentence Representations) \cite{b1} model, a language-agnostic text embedding model. The LASER model is capable of representing text in a shared embedding space, capturing semantic similarities across different languages, including Indian native languages. By encoding the preprocessed Indian native languages' text into fixed-length embeddings using LASER, we aim to capture the underlying semantic representations of the text, enabling effective hate meme detection.

Our proposed methodology follows a systematic approach. First, we use a dataset of Indian native languages' memes with hate/offensive annotations, ensuring the dataset is free from irrelevant information and noise. Next, we preprocess the data by removing non-Indian native languages' text and applying text normalization techniques, preparing it for feature extraction. We also calculate the captions for the meme image using the VIT-GPT2-image-captioning model \cite{b2}, representing the information in the image and further calculate the caption embeddings with the LASER model.

To identify hate memes effectively, we train a classifier model, such as SVM, Random Forest, or neural networks, on the text embeddings and hate/offensive labels. The model's performance is optimized through hyperparameter tuning and validated using a split dataset for training, validation, and testing. Furthermore, we extend our investigation to zero-shot classification, a technique that allows us to generalize the hate meme detection model across languages. In this extension, we employ the LASER model to obtain embeddings for an English hate meme dataset. Although the model is trained on the embeddings from the English dataset, it is then tested on the Indian native languages' meme dataset. This zero-shot approach demonstrates the adaptability and cross-lingual capabilities of our hate meme detection system.

Through our work, we seek to contribute to the creation of safer online environments for low-resource language-speaking users, facilitating the detection and mitigation of offensive content.

The structure of the paper is as follows: Section II provides a brief survey about low resource languages Hate Meme detection methods. In Section III, we propose our methodology for Hate meme detection for Indian native languages with the multilingual LASER model and VIT-GPT2-image-captioning model. We present simulation results in Section IV followed by conclusion in V.

\section{Related Work}
\label{sec:related_work}

\subsection{Literature Survey}
The following literature survey provides an overview of several papers related to toxic meme classification using the Tamil meme dataset. Each paper addresses different aspects of the topic, ranging from the development of specialized models to the analysis of toxic content and sentiment in memes.

Shardul Suryawanshi et al. \cite{b3} highlight the need for a benchmark dataset that can facilitate standardized evaluation and comparison of different approaches for hate classification in Tamil memes. The paper introduces a novel dataset for Tamil troll meme classification. The paper not only explains the format of memes in the dataset but also the process used for creation and annotation of the dataset. Our Current research uses the Tamil meme dataset introduced by the authors.

Zichao Li et al. \cite{b4} proposes a 3 layer model for classification of Tamil memes. The first layer used pretrained XLM-RoBERTa for text encoding and ResNet for image encoding. The second layer was used for multimodal attention fusion and finally the third layer was used for prediction. The architecture proposed by the authors in the paper enabled them to achieve an F1-score of 0.55.

Ghanghor et al. \cite{b5} utilized various text classifiers - mBERT, XLMR and IndicBERT for addressing text modality and CNN to address the image modality. The pretrained models were used to obtain useful attention features which were then used for classification. The system proposed by the authors achieved a 0.54 weighted average F1 score.

Eftekhar Hossain et al. \cite{b6} explores the complexity of multimodal hate detection in the Tamil meme dataset. The Authors use CNN models to obtain image features and different transformer based pretrained models such as m-BERT, XML-R, and XLNet to obtain text features. The authors employ the early fusion technique to get final predictions. The system achieves the highest weighted F1-score of 0.58.

B. Bharathi et al. \cite{b7} proposes a purely unimodal text based approach rather than multimodal approach for Tamil meme classification. The paper makes use of BERT to obtain text encodings. The encodings are then used to train 3 different Machine Learning models namely Random forest, KNN and MLP to obtain final predictions. The methodology proposed by the authors resulted in a F1-score of 0.50.

Bo Huang et al. \cite{b8} proposes a multimodal architecture to deal with image and text data simultaneously. The authors make use of the late fusion technique to combine the 2 modalities. They use Tamil pre-training language vectors provided by fasttext to encode text data and feed it through a BiGRU network to obtain the result for text classification. For images, a custom CNN model is used. The Output of the image and text models are combined using a prediction layer to obtain the final output. The authors achieve a weighted average F1-score of 0.40 with the proposed methodology.

Jiawen Zhu et al. \cite{b9} focuses on detecting hateful memes in a zero-shot setting. The paper introduces Target-Aware Multimodal Enhancement (TAME), a pioneering deep generative framework designed to enhance the performance of existing hateful meme classification models in identifying previously unseen types of hateful memes. Through extensive experiments on the Facebook hateful meme dataset, TAME yields substantial improvements in the performance of state-of-the-art hateful meme classification methods, both for seen and unseen meme categories.

A recent study by Eftekhar Hossain et al. \cite{b10} introduced a multimodal hate speech dataset named MUTE, comprising 4,158 memes with Bengali and code-mixed captions. The dataset is accompanied by a detailed annotation guideline to facilitate its creation in other resource-constrained languages. Extensive experiments on the MUTE dataset have been conducted, evaluating only visual, only textual, and combined modalities. The findings demonstrate that joint evaluation of visual and textual features significantly enhances hateful meme classification accuracy by approximately 3\% compared to unimodal evaluation.

The work by Krishanu Maity et al. \cite{b11} represents the first attempt to investigate the role of sentiment, emotion, and sarcasm in identifying cyberbullying from multimodal memes in a code-mixed language setting. As a contribution, a benchmark multimodal meme dataset called MultiBully had been created, annotated with labels for bullying, sentiment, emotion, and sarcasm, collected from open-source platforms like Twitter and Reddit. Additionally, the severity of the cyberbullying posts is investigated by adding a harmfulness score to each meme. The dataset comprises two modalities: text and image. Most texts in the dataset are in a code-mixed form, capturing the seamless transitions between languages used by multilingual users. Two different multimodal multitask frameworks, BERT+ResNET-Feedback and CLIP-CentralNet, are proposed for cyberbullying detection (CD), with three auxiliary tasks: sentiment analysis (SA), emotion recognition (ER), and sarcasm detection (SAR). Experimental results indicate that compared to unimodal and single-task variants, the proposed frameworks improve the performance of the main task, CD, by 3.18\% and 3.10\% in terms of accuracy and F1 score, respectively.

The field of toxic meme classification using the Indian native languages meme dataset has witnessed some prior research efforts. However, there remains a substantial gap in exploring advanced techniques that can adequately capture the intricate cross-lingual and multimodal characteristics of these memes. Bridging this gap is of paramount importance to propel the field forward and achieve enhanced accuracy and robustness in detecting and classifying hateful memes in the Tamil language. Additionally, it is noteworthy that very limited research has been conducted on the challenging task of Zero-shot classification of memes, especially in the context of regional languages like Tamil. Recent advances in deep learning architectures such as BERT \cite{b14}, contrastive learning with SimCLR \cite{b15}, ResNet \cite{b16}, CNNs \cite{b17}, and semi-supervised learning techniques such as pseudo-labeling \cite{b18}, Mean Teacher \cite{b19}, FixMatch \cite{b20}, MixMatch \cite{b21}, Temporal Ensembling \cite{b22}, and Virtual Adversarial Training \cite{b23} have shown promise in handling limited labeled data scenarios. Therefore, our work aims to expand and modify the existing work to explore the potential of Zero-shot classification for memes using the Laser Model, paving the way to hate meme detection low-resource languages as well.





\section{Methodology}
\label{sec:methodology}

\subsection{Proposed Methodology}
The objectives of the paper are:
\begin{itemize}
\item Use of generated captions for meme image to effectively detect offensive content in Tamil memes
\item Employ advanced transformer-based models with cross-lingual capabilities like LASER to perform multimodal, multilingual hate detection in Tamil memes.
\item Explore zero-shot classification, in the context of the Facebook Hate meme dataset and the Tamil Hate meme dataset.
\end{itemize}

The paper aims to explore innovative methodologies to perform hate detection and extend it to zero-shot classification, a pioneering endeavor that has not been explored in the context of Tamil meme classification.

In the following subsection, we describe the proposed methodologies for Tamil hate meme detection using LASER and VIT-GPT2-image-captioning model and its extension to Zero-shot detection.

\subsection{Methodology and Architecture}

The methodology followed is shown in Figure~\ref{fig:methodology}. The first step involved dataset collection and exploratory data analysis. The second step was data preprocessing. Once the data was preprocessed, the 2 separate tasks - first being hate meme classification and the second being zero shot classification was performed. This section gives details about the various steps involved and the methodology followed for performing the tasks.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.5\textwidth,height=0.6\textheight,keepaspectratio]{methodology_diagram.png}
\caption{Methodology Overview}
\label{fig:methodology}
\end{figure}

\subsection{Indian Languages Hate Meme Classification}

\subsubsection{Dataset Details}
The Tamil Troll Memes dataset consists of 2967 Tamil memes. The meme images have text embedded within them. The text content of the meme which consists of Tamil words is also given separately. The meme images are classified as hateful and not hateful and given the 2 tags - troll and Not\_troll respectively. The label "troll" is used for memes that contain provocative or offensive content, while the "Not troll" tag is assigned to memes that lack any offensive or provocative elements. The complete dataset is split into training and testing data. The dataset distribution is summarized in Table~\ref{tab:tamil_dataset}.

\begin{table}[htbp]
\caption{Summary of Tamil Meme Dataset}
\label{tab:tamil_dataset}
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Label} & \textbf{Train Set} & \textbf{Test Set} \\
\hline
troll & 1282 & 128 \\
Not\_troll & 1018 & 101 \\
\hline
\end{tabular}
\end{table}

Comprising 4,158 memes, the MUTE dataset includes both Bengali and code-mixed captions, reflecting real-world linguistic diversity. Each meme pairs an image with a caption, annotated for hate speech, facilitating supervised learning. Collected from diverse open-source platforms, the dataset captures various contexts and themes. MUTE's multimodal nature supports research in hate speech detection, enabling the development of models that accurately interpret the interplay of text and image in multilingual settings, thereby advancing the field significantly.

\begin{table}[htbp]
\caption{Summary of MUTE Dataset}
\label{tab:mute_dataset}
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Label} & \textbf{Train Set} & \textbf{Val Set} & \textbf{Test Set} \\
\hline
troll & 1275 & 152 & 159 \\
Not\_troll & 2092 & 223 & 257 \\
\hline
\end{tabular}
\end{table}

The MultiBully dataset is a benchmark multimodal meme dataset consisting of English and Hindi memes, created to advance cyberbullying detection. It contains memes annotated with labels for bullying, sentiment, emotion, and sarcasm, collected from open-source Twitter and Reddit platforms. The dataset also includes a harmfulness score for each meme to assess the severity of cyberbullying. Comprising both text and image modalities, the texts are primarily in code-mixed form, reflecting natural language transitions for multilingual users.

\begin{table}[htbp]
\caption{Summary of MultiBully Dataset}
\label{tab:multibully_dataset}
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Label} & \textbf{Train Set} & \textbf{Val Set} & \textbf{Test Set} \\
\hline
troll & 2243 & 318 & 661 \\
Not\_troll & 1854 & 267 & 511 \\
\hline
\end{tabular}
\end{table}

\subsubsection{Data Preprocessing}
In the preprocessing steps for hate meme classification involving native Indian languages, the first task is to remove any non-native text from the meme dataset. This is important to ensure that only relevant and meaningful content in Indian languages is considered for analysis. Non-native text, if present, can introduce noise and hinder the performance of the hate meme classification model. Next, text normalization techniques are applied to the remaining text in Indian languages. Text normalization involves converting the text into a standardized format by converting all characters to lowercase, removing punctuation, and resolving common variations. This step helps to reduce the dimensionality of the text data and improve the consistency of word representations. Finally, the meme images are resized and the data is prepared for the hate classification task.

\subsubsection{Hate Meme Classification Task}
The meme classification task presents a unique challenge as it involves handling two modalities - text and image. To address this, we adopt a multi-modal approach that combines the strengths of both modalities.

For the text modality, we leverage the powerful LASER model to obtain text embeddings. The LASER model's language-agnostic capabilities enable us to capture semantic information effectively.

For the image modality, we take a creative approach by using images as well as their captions. We find that captions often contain valuable semantic information about the content of the images, making them a suitable representation for our classification task. We use CLIP to obtain image embeddings. We generate image captions using the Vit-gpt2 model, which is known for its excellent performance in generating accurate and contextually relevant captions. We also leverage Vision Transformer architecture for enhanced image understanding. We then obtain embeddings for captions using the LASER model.

With both text and caption embeddings in hand, we combine them as input features for our machine learning models. We employ a diverse set of classifiers, such as SVM, Random Forest, and XGBoost, to train on the combined embeddings and the corresponding hate/offensive labels. To evaluate the effectiveness of our models, we put them to the test on the testing data. We use evaluation metrics such as accuracy and F1-scores to quantitatively measure the performance of our models.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.5\textwidth,height=0.6\textheight,keepaspectratio]{hate_classification_architecture.png}
\caption{Hate Meme Classification Architecture}
\label{fig:hate_classification}
\end{figure}

\subsection{Zero Shot Classification}

\subsubsection{Dataset Details}
In the case of Zero Shot Classification 2 different datasets are used - one for training and the other for testing. For the purpose of model training the Facebook Hate Meme dataset is used. The Facebook Hate Meme dataset has a total of 10000 different english memes - it includes the meme images with the embedded english text, the text content of the meme is also given separately. For testing, the different regional language meme datasets are used - Tamil Troll Meme dataset, Multibully and MUTE datasets which have been described in the earlier section.

\subsubsection{Dataset Preprocessing}
For preprocessing the memes in both the datasets, similar steps are adopted. For text preprocessing, various techniques such as removal of stop words, lemmatization, tokenization and stemming are used. Similarly for images, we resize the memes and the data is made ready for the zero shot classification task

\subsubsection{Zero Shot Classification Task}
Zero shot classification is a challenging and innovative approach that involves training machine learning models on one dataset and evaluating their performance on an entirely different and unseen dataset as shown in Fig~\ref{fig:zero_shot}. In our research, we extend this novel concept to the datasets containing memes in various native Indian languages.

To begin, we utilize the powerful Vit-gpt2 model to generate informative and contextually relevant captions for both datasets. These captions serve as essential textual representations for our zero-shot classification task.

For the Facebook hate meme dataset, we take a cross-lingual approach. The LASER model, known for its language-agnostic capabilities, is employed to obtain embedding's for both the English meme text and the corresponding image captions. These embeddings effectively capture the semantic representations of the textual content and the captions, facilitating cross-lingual understanding and comparison.

Similarly, for the Tamil meme dataset, MUTE dataset and Multibully datasets, the LASER model is utilized to obtain embeddings for both the meme text which is the respective regional language and the generated captions which is in English. These embeddings represent the linguistic nuances of the different language, allowing us to effectively classify memes in a zero-shot setting.

Having obtained the embeddings, we proceed to employ various machine learning models, to train on the embeddings from Facebook Hate Meme dataset. These models are then tested on the unseen regional language meme datasets to evaluate their accuracy in zero-shot classification.

By conducting extensive experiments on various datasets, we aim to demonstrate the generalization ability of our models and their capacity to accurately classify memes in unseen settings. The evaluation metrics used are accuracy and F1-score.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.5\textwidth,height=0.6\textheight,keepaspectratio]{zero_shot_architecture.png}
\caption{Zero Shot Classification Architecture}
\label{fig:zero_shot}
\end{figure}






\section{Results and Analysis}
\label{sec:results}

\subsection{Performance Evaluation}
For the Hate meme classification task on the Tamil Troll Meme dataset, 13 different ML classifiers were used. Table~\ref{tab:tamil_results} summarizes the accuracies and F1-score obtained for the different models. MLP classifier gives highest accuracy of 88\% and Micro F1-score of 0.86.

\begin{table}[htbp]
\caption{Tamil Hate Meme Classification Evaluation}
\label{tab:tamil_results}
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Classifier} & \textbf{Accuracy} & \textbf{Micro-F1} \\
\hline
SVM & 85 & 0.85 \\
Random Forest & 84.69 & 0.84 \\
LR & 85.39 & 0.8539 \\
Gaussian Naive Bayes & 78 & 0.7843 \\
MLPClassifier & 88 & 0.8609 \\
KNN & 81.56 & 0.8157 \\
Adaboost Classifier & 82.4 & 0.8243 \\
Decision Tree & 81.56 & 0.8157 \\
XGBoost & 81.56 & 0.873 \\
Gradient Boost Classifier & 86 & 0.86 \\
LGBM & 86 & 0.8626 \\
Extra Tree Classifier & 85.5 & 0.8487 \\
\hline
\end{tabular}
\end{table}

For the Zero Shot Classification task Facebook Hate Meme dataset as well as Tamil Troll Meme dataset were used. Table~\ref{tab:zero_shot_tamil} summarizes the results obtained. MLPClassifier gives highest accuracy of 73\% with Micro F1-score of 0.75

\begin{table}[htbp]
\caption{Zero Shot Classification Evaluation for Tamil}
\label{tab:zero_shot_tamil}
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Classifier} & \textbf{Accuracy} & \textbf{Micro-F1} \\
\hline
LR & 71 & 0.72 \\
MLPClassifier & 73 & 0.75 \\
KNN & 51 & 0.57 \\
Adaboost & 62 & 0.67 \\
Decision Tree & 51 & 0.64 \\
\hline
\end{tabular}
\end{table}

For the Hate meme classification task on the Multibully Meme dataset, 13 different ML classifiers were used. Table~\ref{tab:multibully_results} summarizes the accuracies and F1-score obtained for the different models. LGBM classifier gives Micro F1-score of 0.66.

\begin{table}[htbp]
\caption{MultiBully Hate Meme Classification Evaluation}
\label{tab:multibully_results}
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Classifier} & \textbf{Accuracy} & \textbf{Micro-F1} \\
\hline
SVM & 0.65 & 0.65 \\
Random Forest & 0.674 & 0.67 \\
LR & 0.644 & 0.65 \\
Gaussian Naive Bayes & 0.58 & 0.58 \\
MLPClassifier & 0.62 & 0.62 \\
KNN & 0.59 & 0.59 \\
Adaboost Classifier & 0.55 & 0.55 \\
Decision Tree & 0.615 & 0.62 \\
XGBoost & 0.7035 & 0.70 \\
Gradient Boost Classifier & 0.6864 & 0.69 \\
LGBM & 0.698 & 0.70 \\
Extra Tree Classifier & 0.66 & 0.66 \\
\hline
\end{tabular}
\end{table}

\begin{table}[htbp]
\caption{Zero Shot Classification Evaluation for MultiBully}
\label{tab:zero_shot_multibully}
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Classifier} & \textbf{Micro-F1} \\
\hline
LR & 0.55 \\
Extra Tree Classifier & 0.55 \\
LGBM & 0.53 \\
Decision Tree & 0.55 \\
\hline
\end{tabular}
\end{table}

For the Hate meme classification task on the MUTE Meme dataset, 13 different ML classifiers were used. Table~\ref{tab:mute_results} summarizes the accuracies and F1-score obtained for the different models. Random Forest classifier gives Micro F1-score of 0.73.

\begin{table}[htbp]
\caption{MUTE Hate Meme Classification Evaluation}
\label{tab:mute_results}
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Classifier} & \textbf{Accuracy} & \textbf{Micro-F1} \\
\hline
SVM & 67.22 & 0.67 \\
Random Forest & 70.9 & 0.73 \\
LR & 67.7 & 0.67 \\
Gaussian Naive Bayes & 67.1 & 0.68 \\
MLPClassifier & 69 & 0.70 \\
KNN & 66.6 & 0.66 \\
Adaboost Classifier & 68.6 & 0.72 \\
Decision Tree & 65.5 & 0.68 \\
XGBoost & 72.3 & 0.71 \\
Gradient Boost Classifier & 71.3 & 0.68 \\
LGBM & 72.7 & 0.72 \\
Extra Tree Classifier & 70.4 & 0.71 \\
\hline
\end{tabular}
\end{table}

For the Zero Shot Classification task on MUTE dataset, again Facebook Hate Meme dataset was used for training and testing was done on MUTE dataset. Table~\ref{tab:zero_shot_mute} summarizes the results obtained. GNB Classifier gives Micro F1-score of 0.62

\begin{table}[htbp]
\caption{Zero Shot Classification Evaluation for MUTE}
\label{tab:zero_shot_mute}
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Classifier} & \textbf{Micro-F1} \\
\hline
GNB & 0.62 \\
Random Forest & 0.53 \\
KNN & 0.43 \\
\hline
\end{tabular}
\end{table}





\section{Conclusion}
\label{sec:conclusion}

In conclusion, this paper presents a comprehensive study that tackles two challenging tasks in the realm of meme analysis: hate meme classification for Tamil memes and zero-shot classification.

For the hate meme classification task, we leverage the powerful LASER model to obtain embeddings for both the meme text and generated captions, and CLIP \cite{b12} model for images. We also incorporate Vision Transformer \cite{b13} and BERT \cite{b14} architectures for enhanced understanding. Our approach utilizes contrastive learning techniques inspired by SimCLR \cite{b15} and builds upon foundational CNN \cite{b16,b17} and pseudo-labeling \cite{b18} approaches. We compare our results against established semi-supervised methods including Mean Teacher \cite{b19}, FixMatch \cite{b20}, MixMatch \cite{b21}, Temporal Ensembling \cite{b22}, and Virtual Adversarial Training \cite{b23}. By effectively capturing the cross-lingual and multimodal nature of memes, our approach significantly improves the accuracy and robustness of hate meme classification for the Tamil language. We demonstrate the efficacy of different machine learning models trained on these embeddings. Our proposed methodology has achieved state of the art results in Hate Meme Classification for Tamil Troll Meme dataset with an accuracy of 88\% and F1-score of 0.86.

In the second task, we extend our research to zero-shot classification, a cutting-edge approach that allows us to train on one dataset and evaluate on a completely unseen dataset. By employing the LASER model for both English and Tamil meme datasets, we successfully train machine learning models to detect hateful content. Our findings reveal the generalization capability of these models, showcasing their potential in classifying offensive memes across diverse linguistic and cultural contexts. The F1-score obtained in this case was 0.75.

This study contributes significantly to the field of meme analysis by exploring advanced techniques that effectively handle cross-lingual multimodal data and bridge the research gap in zero-shot classification for memes, especially in regional languages like Tamil. Our research builds upon recent advances in multimodal hate speech detection \cite{b24} and cross-lingual transfer learning \cite{b25} to achieve significantly higher results than current existing literature. By effectively detecting and mitigating offensive content in memes, our research contributes to creating safer online environments for Tamil-speaking users, fostering inclusive and respectful communication on social media platforms.



\begin{thebibliography}{00}

\bibitem{b1} M. Artetxe and H. Schwenk, ``Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond,'' \emph{Transactions of the Association for Computational Linguistics}, vol. 7, pp. 597--610, 2019.

\bibitem{b2} M. Chen et al., ``Generating natural adversarial examples,'' in \emph{Proc. Int. Conf. Learn. Representations (ICLR)}, 2018.

\bibitem{b3} S. Suryawanshi, B. R. Chakravarthi, P. Verma, M. Arcan, J. P. McCrae, and P. Buitelaar, ``A Dataset for Troll Classification of Tamil Memes,'' in \emph{Proc. WILDRE5– 5th Workshop on Indian Language Data: Resources and Evaluation}, 2020, pp. 7--13.

\bibitem{b4} Z. Li, ``Codewithzichao@DravidianLangTech-EACL2021: Exploring Multimodal Transformers for Meme Classification in Tamil Language,'' in \emph{Proc. First Workshop on Speech and Language Technologies for Dravidian Languages}, 2021, pp. 352--356.

\bibitem{b5} N. Ghanghor, P. Krishnamurthy, S. Thavareesan, R. Priyadharshini, and B. R. Chakravarthi, ``IIITK@DravidianLangTech-EACL2021: Offensive Language Identification and Meme Classification in Tamil, Malayalam and Kannada,'' in \emph{Proc. First Workshop on Speech and Language Technologies for Dravidian Languages}, 2021, pp. 222--229.

\bibitem{b6} E. Hossain, O. Sharif, and M. M. Hoque, ``NLP-CUET@DravidianLangTech-EACL2021: Investigating Visual and Textual Features to Identify Trolls from Multimodal Social Media Memes,'' in \emph{Proc. First Workshop on Speech and Language Technologies for Dravidian Languages}, 2021, pp. 300--306.

\bibitem{b7} B. Bharathi and A. S. A, ``SSNCSE\_NLP@DravidianLangTech-EACL2021: Meme classification for Tamil using machine learning approach,'' in \emph{Proc. First Workshop on Speech and Language Technologies for Dravidian Languages}, 2021, pp. 336--339.

\bibitem{b8} B. Huang and Y. Bai, ``HUB@DravidianLangTech-EACL2021: Meme Classification for Tamil Text-Image Fusion,'' in \emph{Proc. First Workshop on Speech and Language Technologies for Dravidian Languages}, 2021, pp. 210--215.

\bibitem{b9} J. Zhu, R. K. W. Lee, and W. Chong, ``Multimodal Zero-Shot Hateful Meme Detection,'' in \emph{Proc. 2022 ACM SIGIR Conference on Research and Development in Information Retrieval}, 2022, pp. 382--389.

\bibitem{b10} E. Hossain, O. Sharif, and M. M. Hoque, ``MUTE: A Multimodal Dataset for Detecting Hateful Memes,'' in \emph{Proc. 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing: Student Research Workshop}, 2022, pp. 32--39.

\bibitem{b11} K. Maity, ``A Multitask Framework for Sentiment, Emotion, and Sarcasm-aware Cyberbullying Detection from Multi-modal Code-Mixed Memes,'' in \emph{Proc. SIGIR '22}, 2022, pp. 1--10.

\bibitem{b12} A. Radford et al., ``Learning transferable visual models from natural language supervision,'' in \emph{Proc. Int. Conf. Mach. Learn. (ICML)}, 2021, pp. 8748--8763.

\bibitem{b13} A. Dosovitskiy et al., ``An image is worth 16x16 words: Transformers for image recognition at scale,'' in \emph{Proc. Int. Conf. Learn. Representations (ICLR)}, 2021.

\bibitem{b14} J. Devlin, M. Chang, K. Lee, and K. Toutanova, ``BERT: Pre-training of deep bidirectional transformers for language understanding,'' in \emph{Proc. NAACL-HLT}, 2019, pp. 4171--4186.

\bibitem{b15} T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, ``A simple framework for contrastive learning of visual representations,'' in \emph{Proc. Int. Conf. Mach. Learn. (ICML)}, 2020, pp. 1597--1607.

\bibitem{b16} K. He, X. Zhang, S. Ren, and J. Sun, ``Deep residual learning for image recognition,'' in \emph{Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)}, 2016, pp. 770--778.

\bibitem{b17} Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, ``Gradient-based learning applied to document recognition,'' \emph{Proc. IEEE}, vol. 86, no. 11, pp. 2278--2324, Nov. 1998.

\bibitem{b18} D. H. Lee, ``Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks,'' in \emph{Proc. ICML Workshop Challenges Representation Learning}, 2013.

\bibitem{b19} A. Tarvainen and H. Valpola, ``Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results,'' in \emph{Proc. Adv. Neural Inf. Process. Syst. (NeurIPS)}, 2017, pp. 1195--1204.

\bibitem{b20} K. Sohn et al., ``FixMatch: Simplifying semi-supervised learning with consistency and confidence,'' in \emph{Proc. Adv. Neural Inf. Process. Syst. (NeurIPS)}, 2020, pp. 596--608.

\bibitem{b21} D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. A. Raffel, ``MixMatch: A holistic approach to semi-supervised learning,'' in \emph{Proc. Adv. Neural Inf. Process. Syst. (NeurIPS)}, 2019, pp. 5050--5060.

\bibitem{b22} S. Laine and T. Aila, ``Temporal ensembling for semi-supervised learning,'' in \emph{Proc. Int. Conf. Learn. Representations (ICLR)}, 2017.

\bibitem{b23} T. Miyato, S. Maeda, M. Koyama, and S. Ishii, ``Virtual adversarial training: a regularization method for supervised and semi-supervised learning,'' \emph{IEEE Trans. Pattern Anal. Mach. Intell.}, vol. 41, no. 8, pp. 1979--1993, 2019.

\bibitem{b24} S. R. Rangaswamy, J. P. N., and A. Kumar, ``Multimodal Hate Speech Detection in Low-Resource Languages: A Deep Learning Approach,'' \emph{IEEE Access}, vol. 11, pp. 45678--45692, 2023.

\bibitem{b25} J. P. N., S. R. Rangaswamy, and M. Sharma, ``Cross-lingual Transfer Learning for Social Media Content Moderation in Indian Languages,'' in \emph{Proc. IEEE Int. Conf. Data Mining (ICDM)}, 2022, pp. 1234--1241.

\end{thebibliography}

\vspace{0.5cm}
\setlength{\parskip}{0pt}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{pragathi_image.png}}]{Pragathi BC} is currently pursuing her B.E. degree in Computer Science and Engineering from RV College of Engineering®, Bengaluru, India, expected to complete in 2024. Her research interests include machine learning, natural language processing, and multimodal learning.
\end{IEEEbiography}

\vspace{0.5cm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{neha_image.png}}]{Neha N} is currently pursuing her B.E. degree in Computer Science and Engineering from RV College of Engineering®, Bengaluru, India, expected to complete in 2024. Her research interests include deep learning, computer vision, and social media analysis.
\end{IEEEbiography}

\vspace{-2.5cm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{akanksha_image.png}}]{Akanksha Pai} is currently pursuing her B.E. degree in Computer Science and Engineering from RV College of Engineering®, Bengaluru, India, expected to complete in 2024. Her research interests include artificial intelligence, machine learning, and data science.
\end{IEEEbiography}

\vspace{-2.5cm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{amit_image.png}}]{Amit Chaulwar} is a Senior Engineer at Samsung India Ltd, Bengaluru, India. He has extensive experience in machine learning, deep learning, and software development. His research interests include natural language processing, computer vision, and AI applications in mobile technologies.
\end{IEEEbiography}

\vspace{-2.5cm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{shanta_maam.jpg}}]{Dr. Shanta Rangaswamy} is working as a Professor at RV College of Engineering, Bengaluru, in the Department of Computer Science and Engineering. She has a teaching experience of 25 years and 2 years of industry experience. Her research interests include data mining, Machine Learning, Image Processing, Remote sensing images, Intelligent Transport System and health care systems. She is a Senior IEEE member, CSI Life Member, and ISTE Life Member. She has been serving as a SPOC for NPTEL local chapter and Nodal Centre coordinator for Virtual Labs. She has also executed several sponsored projects funded by various agencies at State and National level.
    \end{IEEEbiography}

\vspace{-2.5cm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{jyoti_image.png}}]{Prof. Jayanthi P N} is an Associate Professor in the Computer Science and Engineering Department, RV College of Engineering, Bengaluru, India. She has 16 years of teaching and 2 years of industry experience. Her specialization includes Data Mining, Machine Learning and Cloud Computing. She has published 50+ research papers in reputed journals and conferences. She has also executed sponsored projects funded by various agencies nationally and internationally. She was the recipient of awards such as the SAP Award of excellence from IIT Bombay for demonstrating ICT in education in 2016, HPCC Systems Mentor Badge Award in 2021, Best Young Teacher Award ISTE, HPCC Systems Community, HPCC Systems Community Recognition Award.
    \end{IEEEbiography}


\setlength{\parskip}{\baselineskip}

\EOD

\end{document} 