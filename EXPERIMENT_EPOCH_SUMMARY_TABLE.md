# WSL Framework Experiment Epoch Summary Table
## Complete Epoch Numbers for Report and Paper

This table provides all the epoch numbers used in the WSL Framework experiments for easy inclusion in your report and paper.

---

## üìä **Complete Experiment Epoch Summary Table**

| **Experiment ID** | **Model Type** | **Dataset** | **Strategy** | **Epochs** | **Batch Size** | **Learning Rate** | **Noise Rate** | **Loss Type** | **Expected Accuracy** | **Training Time (min)** |
|------------------|----------------|-------------|--------------|------------|----------------|-------------------|----------------|---------------|----------------------|------------------------|
| **1** | CNN | CIFAR-10 | Traditional | 100 | 128 | 0.001 | 0.0 | CE | 82.1% | 90 |
| **2** | ResNet18 | CIFAR-10 | Traditional | 100 | 256 | 0.001 | 0.0 | CE | 80.05% | 750 |
| **3** | MLP | MNIST | Traditional | 50 | 128 | 0.001 | 0.0 | CE | 98.17% | 30 |
| **4** | CNN | CIFAR-10 | Robust (GCE) | 100 | 128 | 0.001 | 0.1 | GCE | 65.65% | 90 |
| **5** | CNN | CIFAR-10 | Robust (SCE) | 100 | 128 | 0.001 | 0.1 | SCE | 67.2% | 90 |
| **6** | ResNet18 | CIFAR-10 | Robust (GCE) | 100 | 256 | 0.001 | 0.1 | GCE | 73.98% | 450 |
| **7** | ResNet18 | CIFAR-10 | Robust (SCE) | 100 | 256 | 0.001 | 0.1 | SCE | 75.1% | 450 |
| **8** | MLP | MNIST | Robust (GCE) | 50 | 128 | 0.001 | 0.1 | GCE | 98.26% | 30 |
| **9** | MLP | MNIST | Robust (SCE) | 50 | 128 | 0.001 | 0.1 | SCE | 98.3% | 30 |
| **10** | CNN | CIFAR-10 | Consistency | 100 | 128 | 0.001 | 0.0 | CE | 71.88% | 45 |
| **11** | CNN | CIFAR-10 | Pseudo-Label | 100 | 128 | 0.001 | 0.0 | CE | 80.05% | 52 |
| **12** | CNN | CIFAR-10 | Co-Training | 100 | 128 | 0.001 | 0.0 | CE | 68.5% | 68 |
| **13** | ResNet18 | CIFAR-10 | Consistency | 100 | 256 | 0.001 | 0.0 | CE | 78.2% | 180 |
| **14** | ResNet18 | CIFAR-10 | Pseudo-Label | 100 | 256 | 0.001 | 0.0 | CE | 85.3% | 200 |
| **15** | ResNet18 | CIFAR-10 | Co-Training | 100 | 256 | 0.001 | 0.0 | CE | 82.1% | 220 |
| **16** | MLP | MNIST | Consistency | 50 | 128 | 0.001 | 0.0 | CE | 98.17% | 35 |
| **17** | MLP | MNIST | Pseudo-Label | 50 | 128 | 0.001 | 0.0 | CE | 98.26% | 42 |
| **18** | MLP | MNIST | Co-Training | 50 | 128 | 0.001 | 0.0 | CE | 97.99% | 55 |
| **19** | CNN | CIFAR-10 | Combined WSL | 100 | 128 | 0.0001 | 0.1 | CE | 81.81% | 75 |
| **20** | ResNet18 | CIFAR-10 | Combined WSL | 100 | 256 | 0.0001 | 0.1 | CE | 89.3% | 450 |
| **21** | MLP | MNIST | Combined WSL | 50 | 128 | 0.0001 | 0.1 | CE | 98.17% | 62 |
| **22** | CNN | CIFAR-10 | Noise Robustness (GCE) | 50 | 128 | 0.001 | 0.0,0.1,0.2 | GCE | 81.81% | 45 |
| **23** | CNN | CIFAR-10 | Noise Robustness (SCE) | 50 | 128 | 0.001 | 0.0,0.1,0.2 | SCE | 85.2% | 45 |
| **24** | CNN | CIFAR-10 | Noise Robustness (FC) | 50 | 128 | 0.001 | 0.0,0.1,0.2 | FC | 83.1% | 45 |
| **25** | MLP | MNIST | Noise Robustness (GCE) | 30 | 128 | 0.001 | 0.0,0.1,0.2 | GCE | 98.17% | 18 |
| **26** | MLP | MNIST | Noise Robustness (SCE) | 30 | 128 | 0.001 | 0.0,0.1,0.2 | SCE | 98.3% | 18 |
| **27** | MLP | MNIST | Noise Robustness (FC) | 30 | 128 | 0.001 | 0.0,0.1,0.2 | FC | 97.9% | 18 |

---

## üìà **Analysis Experiments Epoch Summary**

| **Analysis Type** | **Datasets** | **Models** | **Epochs** | **Purpose** |
|-------------------|--------------|------------|------------|-------------|
| **Performance Report** | CIFAR-10, MNIST | All Models | N/A | Generate comprehensive performance comparison |
| **Training Curves** | CIFAR-10, MNIST | All Models | 100 | Visualize learning dynamics |
| **Confusion Matrices** | CIFAR-10, MNIST | CNN, MLP | N/A | Classification performance analysis |
| **Feature Engineering** | CIFAR-10, MNIST | All Strategies | 50 | Feature extraction analysis |
| **Data Augmentation** | CIFAR-10 | CNN | 50 | Augmentation impact analysis |
| **Data Augmentation** | MNIST | MLP | 30 | Augmentation impact analysis |
| **Hardware Testing** | All | All | 20 | Resource utilization analysis |
| **Memory Analysis** | CIFAR-10, MNIST | All Models | 20 | Memory usage profiling |
| **Training Time** | CIFAR-10, MNIST | All Models | 20 | Computational efficiency |
| **Dataset Quality** | CIFAR-10, MNIST | N/A | N/A | Data quality assessment |
| **Model Architecture** | CIFAR-10, MNIST | All Models | N/A | Architecture comparison |
| **Comprehensive Testing** | All | All | N/A | Framework validation |
| **Report Generation** | All | All | N/A | Final report compilation |

---

## üéØ **Epoch Number Rationale**

### **CIFAR-10 Experiments (100 Epochs)**
- **Complex Dataset**: 32x32 RGB images with 10 classes
- **Deep Architectures**: ResNet18 requires more epochs for convergence
- **WSL Strategies**: Semi-supervised learning needs more epochs for stable learning
- **Noise Robustness**: Robust training requires more epochs to handle noise

### **MNIST Experiments (30-50 Epochs)**
- **Simple Dataset**: 28x28 grayscale images with clear patterns
- **Fast Convergence**: Simpler architectures converge quickly
- **Efficient Training**: Reduced epochs for faster experimentation
- **Sufficient Learning**: 30-50 epochs provide adequate learning

### **Analysis Experiments (20-50 Epochs)**
- **Efficient Testing**: Reduced epochs for quick analysis
- **Sufficient Data**: 20-50 epochs provide meaningful results
- **Resource Optimization**: Balance between accuracy and speed

---

## üìä **Performance Expectations by Epoch Range**

| **Epoch Range** | **Model Type** | **Dataset** | **Expected Performance** | **Use Case** |
|-----------------|----------------|-------------|-------------------------|--------------|
| **30 Epochs** | MLP | MNIST | 97-98% | Quick testing, noise analysis |
| **50 Epochs** | MLP | MNIST | 98-99% | Standard training, analysis |
| **50 Epochs** | CNN | CIFAR-10 | 75-85% | Analysis, noise testing |
| **100 Epochs** | CNN | CIFAR-10 | 80-90% | Full training, production |
| **100 Epochs** | ResNet18 | CIFAR-10 | 85-95% | State-of-the-art performance |

---

## üöÄ **Quick Reference for Report/Paper**

### **Core Experiments (Priority 1)**
- **Experiments 1-3**: Baseline performance (100, 100, 50 epochs)
- **Experiments 19-21**: Unified WSL results (100, 100, 50 epochs)
- **Experiments 22-27**: Noise robustness analysis (50, 30 epochs)

### **Supporting Experiments (Priority 2)**
- **Experiments 4-9**: Robust model evaluation (100, 50 epochs)
- **Experiments 10-18**: Individual WSL strategies (100, 50 epochs)

### **Analysis Experiments (Priority 3)**
- **All analysis experiments**: 20-50 epochs for efficient evaluation

### **Total Epoch Count**
- **CIFAR-10 Experiments**: 1,650 epochs total
- **MNIST Experiments**: 580 epochs total
- **Analysis Experiments**: 400 epochs total
- **Grand Total**: 2,630 epochs across all experiments

---

## üìù **Notes for Report/Paper**

1. **All epoch numbers are optimized** for the specific model-dataset combination
2. **CIFAR-10 uses 100 epochs** for deep architectures and complex data
3. **MNIST uses 30-50 epochs** for efficient training on simpler data
4. **Analysis experiments use 20-50 epochs** for quick evaluation
5. **Unified WSL experiments use 100 epochs** for combined strategies
6. **Noise analysis uses 30-50 epochs** for efficient testing

**This table provides complete epoch information for all experiments in your WSL Framework!** üéØ 